{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OrderBook Accuracy Analysis\n",
    "\n",
    "This notebook analyzes the accuracy of OrderBook floor price predictions compared to actual subsequent sales.\n",
    "\n",
    "## Key Questions\n",
    "1. **How accurate are OrderBook predictions?** (MAE, RMSE, % error)\n",
    "2. **Is confidence calibrated?** (Do high-confidence predictions perform better?)\n",
    "3. **What are the failure modes?** (When does OrderBook fail?)\n",
    "4. **OrderBook vs Sales Fallback?** (Which source is more accurate?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Load backtest data\n",
    "DATA_PATH = Path('../data/orderbook_backtest.csv')\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_csv(DATA_PATH, parse_dates=['prediction_date', 'next_sale_date'])\n",
    "    print(f\"Loaded {len(df)} observations\")\n",
    "    print(f\"Date range: {df['prediction_date'].min()} to {df['prediction_date'].max()}\")\n",
    "else:\n",
    "    print(f\"Data file not found: {DATA_PATH}\")\n",
    "    print(\"Run: python scripts/backtest_orderbook.py --days 90\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data):\n",
    "    \"\"\"Calculate accuracy metrics for a dataset.\"\"\"\n",
    "    errors = data['error']\n",
    "    abs_errors = data['absolute_error']\n",
    "    pct_errors = data['percentage_error']\n",
    "    \n",
    "    return {\n",
    "        'count': len(data),\n",
    "        'mae': abs_errors.mean(),\n",
    "        'rmse': np.sqrt((errors ** 2).mean()),\n",
    "        'median_error': errors.median(),\n",
    "        'median_abs_error': abs_errors.median(),\n",
    "        'median_pct_error': pct_errors.median(),\n",
    "        'std_error': errors.std(),\n",
    "        'p95_abs_error': abs_errors.quantile(0.95),\n",
    "        'overestimate_pct': (errors > 0).mean() * 100,\n",
    "    }\n",
    "\n",
    "if 'df' in dir():\n",
    "    overall = calculate_metrics(df)\n",
    "    print(\"=\"*60)\n",
    "    print(\"OVERALL ACCURACY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Observations:      {overall['count']:,}\")\n",
    "    print(f\"MAE:               ${overall['mae']:.2f}\")\n",
    "    print(f\"RMSE:              ${overall['rmse']:.2f}\")\n",
    "    print(f\"Median Error:      ${overall['median_error']:.2f}\")\n",
    "    print(f\"Median Abs Error:  ${overall['median_abs_error']:.2f}\")\n",
    "    print(f\"Median % Error:    {overall['median_pct_error']:.1f}%\")\n",
    "    print(f\"95th %ile Error:   ${overall['p95_abs_error']:.2f}\")\n",
    "    print(f\"Overestimate %:    {overall['overestimate_pct']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Error distribution\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(df['error'], bins=50, edgecolor='white', alpha=0.7)\n",
    "    ax1.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel('Error (Predicted - Actual)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Error Distribution')\n",
    "    \n",
    "    # Absolute error distribution\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(df['absolute_error'], bins=50, edgecolor='white', alpha=0.7, color='orange')\n",
    "    ax2.axvline(overall['mae'], color='red', linestyle='--', linewidth=2, label=f'MAE=${overall[\"mae\"]:.2f}')\n",
    "    ax2.set_xlabel('Absolute Error ($)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Absolute Error Distribution')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Percentage error distribution\n",
    "    ax3 = axes[2]\n",
    "    ax3.hist(df['percentage_error'].clip(upper=100), bins=50, edgecolor='white', alpha=0.7, color='green')\n",
    "    ax3.axvline(overall['median_pct_error'], color='red', linestyle='--', linewidth=2, label=f'Median={overall[\"median_pct_error\"]:.1f}%')\n",
    "    ax3.set_xlabel('Percentage Error (%)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Percentage Error Distribution')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confidence Calibration\n",
    "\n",
    "Do high-confidence predictions actually perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    # Create confidence buckets\n",
    "    df['confidence_bucket'] = pd.cut(\n",
    "        df['confidence'], \n",
    "        bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        labels=['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics by confidence bucket\n",
    "    conf_metrics = df.groupby('confidence_bucket', observed=True).agg({\n",
    "        'absolute_error': ['count', 'mean', 'median'],\n",
    "        'percentage_error': 'median',\n",
    "        'error': lambda x: (x > 0).mean() * 100  # Overestimate %\n",
    "    }).round(2)\n",
    "    conf_metrics.columns = ['Count', 'MAE', 'Median Abs Err', 'Median % Err', 'Overestimate %']\n",
    "    \n",
    "    print(\"Accuracy by Confidence Level\")\n",
    "    print(\"=\"*70)\n",
    "    print(conf_metrics.to_string())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    bucket_mids = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    maes = conf_metrics['MAE'].values\n",
    "    counts = conf_metrics['Count'].values\n",
    "    \n",
    "    bars = ax.bar(bucket_mids, maes, width=0.15, alpha=0.7)\n",
    "    ax.set_xlabel('Confidence Score')\n",
    "    ax.set_ylabel('Mean Absolute Error ($)')\n",
    "    ax.set_title('Confidence Calibration: Does Higher Confidence = Lower Error?')\n",
    "    ax.set_xticks(bucket_mids)\n",
    "    ax.set_xticklabels(['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0'])\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'n={int(count)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calibration assessment\n",
    "    if len(maes) >= 2:\n",
    "        is_calibrated = maes[-1] < maes[0]  # High confidence should have lower MAE\n",
    "        print(f\"\\nCalibration Assessment: {'PASS' if is_calibrated else 'FAIL'}\")\n",
    "        print(f\"  Low confidence MAE: ${maes[0]:.2f}\")\n",
    "        print(f\"  High confidence MAE: ${maes[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OrderBook vs Sales Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    source_metrics = df.groupby('source').apply(calculate_metrics).apply(pd.Series)\n",
    "    \n",
    "    print(\"Accuracy by Prediction Source\")\n",
    "    print(\"=\"*70)\n",
    "    print(source_metrics[['count', 'mae', 'rmse', 'median_abs_error', 'median_pct_error', 'overestimate_pct']].round(2).to_string())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # MAE comparison\n",
    "    ax1 = axes[0]\n",
    "    sources = source_metrics.index.tolist()\n",
    "    maes = source_metrics['mae'].values\n",
    "    ax1.bar(sources, maes, color=['#4ecdc4', '#ff6b6b'])\n",
    "    ax1.set_ylabel('Mean Absolute Error ($)')\n",
    "    ax1.set_title('MAE by Source')\n",
    "    for i, (src, mae) in enumerate(zip(sources, maes)):\n",
    "        ax1.text(i, mae + 0.3, f'${mae:.2f}', ha='center')\n",
    "    \n",
    "    # Box plots\n",
    "    ax2 = axes[1]\n",
    "    df.boxplot(column='absolute_error', by='source', ax=ax2)\n",
    "    ax2.set_ylabel('Absolute Error ($)')\n",
    "    ax2.set_title('Error Distribution by Source')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Price-Dependent Accuracy\n",
    "\n",
    "Is accuracy different for high-value vs low-value cards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    # Create price buckets\n",
    "    df['price_bucket'] = pd.cut(\n",
    "        df['next_sale_price'], \n",
    "        bins=[0, 10, 25, 50, 100, float('inf')],\n",
    "        labels=['$0-10', '$10-25', '$25-50', '$50-100', '$100+']\n",
    "    )\n",
    "    \n",
    "    price_metrics = df.groupby('price_bucket', observed=True).agg({\n",
    "        'absolute_error': ['count', 'mean', 'median'],\n",
    "        'percentage_error': 'median',\n",
    "    }).round(2)\n",
    "    price_metrics.columns = ['Count', 'MAE', 'Median Abs Err', 'Median % Err']\n",
    "    \n",
    "    print(\"Accuracy by Price Range\")\n",
    "    print(\"=\"*60)\n",
    "    print(price_metrics.to_string())\n",
    "    \n",
    "    # Scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    scatter = ax.scatter(\n",
    "        df['next_sale_price'], \n",
    "        df['absolute_error'],\n",
    "        c=df['confidence'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.5,\n",
    "        s=20\n",
    "    )\n",
    "    ax.set_xlabel('Actual Sale Price ($)')\n",
    "    ax.set_ylabel('Absolute Error ($)')\n",
    "    ax.set_title('Error vs Price (colored by confidence)')\n",
    "    plt.colorbar(scatter, label='Confidence')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df['next_sale_price'], df['absolute_error'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(df['next_sale_price'].min(), df['next_sale_price'].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.8, label=f'Trend')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Listing Count Impact\n",
    "\n",
    "How many listings are needed for accurate predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    # Create listing count buckets\n",
    "    df['listing_bucket'] = pd.cut(\n",
    "        df['total_listings'], \n",
    "        bins=[0, 3, 5, 10, 20, float('inf')],\n",
    "        labels=['1-3', '4-5', '6-10', '11-20', '20+']\n",
    "    )\n",
    "    \n",
    "    listing_metrics = df.groupby('listing_bucket', observed=True).agg({\n",
    "        'absolute_error': ['count', 'mean', 'median'],\n",
    "        'percentage_error': 'median',\n",
    "    }).round(2)\n",
    "    listing_metrics.columns = ['Count', 'MAE', 'Median Abs Err', 'Median % Err']\n",
    "    \n",
    "    print(\"Accuracy by Number of Listings\")\n",
    "    print(\"=\"*60)\n",
    "    print(listing_metrics.to_string())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    buckets = listing_metrics.index.tolist()\n",
    "    maes = listing_metrics['MAE'].values\n",
    "    counts = listing_metrics['Count'].values\n",
    "    \n",
    "    bars = ax.bar(range(len(buckets)), maes, color='#4ecdc4')\n",
    "    ax.set_xlabel('Number of Listings')\n",
    "    ax.set_ylabel('Mean Absolute Error ($)')\n",
    "    ax.set_title('Does More Data = Better Predictions?')\n",
    "    ax.set_xticks(range(len(buckets)))\n",
    "    ax.set_xticklabels(buckets)\n",
    "    \n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "                f'n={int(count)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time-to-Sale Impact\n",
    "\n",
    "Are predictions less accurate when sales happen days/weeks later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    # Create time-to-sale buckets\n",
    "    df['days_bucket'] = pd.cut(\n",
    "        df['days_to_sale'], \n",
    "        bins=[-1, 0, 1, 3, 7, 14, float('inf')],\n",
    "        labels=['Same day', '1 day', '2-3 days', '4-7 days', '8-14 days', '14+ days']\n",
    "    )\n",
    "    \n",
    "    time_metrics = df.groupby('days_bucket', observed=True).agg({\n",
    "        'absolute_error': ['count', 'mean', 'median'],\n",
    "        'percentage_error': 'median',\n",
    "    }).round(2)\n",
    "    time_metrics.columns = ['Count', 'MAE', 'Median Abs Err', 'Median % Err']\n",
    "    \n",
    "    print(\"Accuracy by Time to Next Sale\")\n",
    "    print(\"=\"*60)\n",
    "    print(time_metrics.to_string())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    buckets = time_metrics.index.tolist()\n",
    "    maes = time_metrics['MAE'].values\n",
    "    \n",
    "    ax.bar(range(len(buckets)), maes, color='#ff6b6b')\n",
    "    ax.set_xlabel('Days Until Next Sale')\n",
    "    ax.set_ylabel('Mean Absolute Error ($)')\n",
    "    ax.set_title('Prediction Accuracy Over Time')\n",
    "    ax.set_xticks(range(len(buckets)))\n",
    "    ax.set_xticklabels(buckets, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Worst Predictions (Edge Cases)\n",
    "\n",
    "What went wrong in the biggest failures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    worst = df.nlargest(10, 'absolute_error')[[\n",
    "        'card_name', 'treatment', 'predicted_floor', 'next_sale_price',\n",
    "        'error', 'confidence', 'source', 'total_listings'\n",
    "    ]]\n",
    "    \n",
    "    print(\"Top 10 Worst Predictions\")\n",
    "    print(\"=\"*80)\n",
    "    print(worst.to_string(index=False))\n",
    "    \n",
    "    # Common patterns in failures\n",
    "    print(\"\\n\\nFailure Mode Analysis (Top 20% worst predictions):\")\n",
    "    print(\"=\"*60)\n",
    "    threshold = df['absolute_error'].quantile(0.80)\n",
    "    failures = df[df['absolute_error'] >= threshold]\n",
    "    \n",
    "    print(f\"\\nFailures with low listing count (<5): {(failures['total_listings'] < 5).sum()} ({(failures['total_listings'] < 5).mean()*100:.1f}%)\")\n",
    "    print(f\"Failures with sales_fallback: {(failures['source'] == 'sales_fallback').sum()} ({(failures['source'] == 'sales_fallback').mean()*100:.1f}%)\")\n",
    "    print(f\"Failures with high confidence (>0.6): {(failures['confidence'] > 0.6).sum()} ({(failures['confidence'] > 0.6).mean()*100:.1f}%)\")\n",
    "    print(f\"Overestimates: {(failures['error'] > 0).sum()} ({(failures['error'] > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    print(\"=\"*60)\n",
    "    print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Key findings\n",
    "    print(\"\\n1. OVERALL ACCURACY\")\n",
    "    print(f\"   - MAE: ${overall['mae']:.2f}\")\n",
    "    print(f\"   - Median % Error: {overall['median_pct_error']:.1f}%\")\n",
    "    quality = \"GOOD\" if overall['median_pct_error'] < 15 else \"NEEDS IMPROVEMENT\" if overall['median_pct_error'] < 30 else \"POOR\"\n",
    "    print(f\"   - Quality: {quality}\")\n",
    "    \n",
    "    # Confidence calibration\n",
    "    print(\"\\n2. CONFIDENCE CALIBRATION\")\n",
    "    high_conf = df[df['confidence'] >= 0.6]['absolute_error'].mean()\n",
    "    low_conf = df[df['confidence'] < 0.4]['absolute_error'].mean()\n",
    "    is_calibrated = high_conf < low_conf\n",
    "    print(f\"   - High confidence MAE: ${high_conf:.2f}\")\n",
    "    print(f\"   - Low confidence MAE: ${low_conf:.2f}\")\n",
    "    print(f\"   - Calibration: {'GOOD' if is_calibrated else 'POOR - High confidence predictions are not better!'}\")\n",
    "    \n",
    "    # Source comparison\n",
    "    print(\"\\n3. ORDERBOOK VS SALES FALLBACK\")\n",
    "    ob_data = df[df['source'] == 'order_book']\n",
    "    sf_data = df[df['source'] == 'sales_fallback']\n",
    "    if len(ob_data) > 0 and len(sf_data) > 0:\n",
    "        print(f\"   - OrderBook MAE: ${ob_data['absolute_error'].mean():.2f} (n={len(ob_data)})\")\n",
    "        print(f\"   - Sales Fallback MAE: ${sf_data['absolute_error'].mean():.2f} (n={len(sf_data)})\")\n",
    "        better = \"OrderBook\" if ob_data['absolute_error'].mean() < sf_data['absolute_error'].mean() else \"Sales Fallback\"\n",
    "        print(f\"   - Better source: {better}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n4. RECOMMENDATIONS\")\n",
    "    if not is_calibrated:\n",
    "        print(\"   - [ ] Fix confidence algorithm - high confidence should = low error\")\n",
    "    if overall['median_pct_error'] > 20:\n",
    "        print(\"   - [ ] Improve bucket algorithm for better floor estimation\")\n",
    "    if overall['overestimate_pct'] > 60:\n",
    "        print(\"   - [ ] Algorithm tends to OVERESTIMATE - consider bias correction\")\n",
    "    elif overall['overestimate_pct'] < 40:\n",
    "        print(\"   - [ ] Algorithm tends to UNDERESTIMATE - consider bias correction\")\n",
    "    print(\"   - [ ] Keep OrderBook as FALLBACK, not primary pricing source\")\n",
    "    print(\"   - [ ] Use sales-based floor as primary when available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
