{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Regressive Fair Market Price Model\n",
    "\n",
    "Build a pricing model that predicts fair market value using:\n",
    "- **Historical sales** (auto-regressive features)\n",
    "- **Related cards** (same set, rarity, treatment)\n",
    "- **Liquidity metrics** (sales velocity, days since last sale)\n",
    "- **Order book depth** (active listings, spread)\n",
    "- **Treatment multipliers** (learned from data)\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "```\n",
    "FMP(card, treatment, t) = \n",
    "    α₀ + \n",
    "    α₁ × price(t-1) +           # AR(1) - last sale\n",
    "    α₂ × price(t-7d) +          # Weekly lag\n",
    "    α₃ × floor_4_lowest +       # Current floor\n",
    "    β₁ × treatment_mult +       # Treatment effect\n",
    "    β₂ × rarity_mult +          # Rarity effect\n",
    "    γ₁ × liquidity_score +      # How often it sells\n",
    "    γ₂ × depth_score +          # Order book depth\n",
    "    δ₁ × related_cards_avg      # Similar cards' prices\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from sqlalchemy import text\n",
    "from sqlmodel import Session\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from app.db import engine\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Fetch all sales with card metadata, treatment info, and temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sales_data(days: int = 180) -> pd.DataFrame:\n",
    "    \"\"\"Fetch all sales with card metadata.\"\"\"\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(days=days)\n",
    "    \n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            mp.id as sale_id,\n",
    "            mp.card_id,\n",
    "            c.name as card_name,\n",
    "            c.card_number,\n",
    "            r.name as rarity,\n",
    "            c.set_name,\n",
    "            COALESCE(NULLIF(mp.product_subtype, ''), mp.treatment) as treatment,\n",
    "            mp.price,\n",
    "            COALESCE(mp.sold_date, mp.scraped_at) as sale_date,\n",
    "            mp.platform,\n",
    "            mp.is_bulk_lot\n",
    "        FROM marketprice mp\n",
    "        JOIN card c ON c.id = mp.card_id\n",
    "        LEFT JOIN rarity r ON r.id = c.rarity_id\n",
    "        WHERE mp.listing_type = 'sold'\n",
    "          AND mp.is_bulk_lot = FALSE\n",
    "          AND COALESCE(mp.sold_date, mp.scraped_at) >= :cutoff\n",
    "          AND mp.price > 0\n",
    "          AND mp.price < 10000  -- Filter obvious errors\n",
    "        ORDER BY mp.card_id, sale_date\n",
    "    \"\"\")\n",
    "    \n",
    "    with Session(engine) as session:\n",
    "        result = session.execute(query, {\"cutoff\": cutoff})\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        'sale_id', 'card_id', 'card_name', 'card_number', 'rarity', \n",
    "        'set_name', 'treatment', 'price', 'sale_date', 'platform', 'is_bulk_lot'\n",
    "    ])\n",
    "    \n",
    "    df['sale_date'] = pd.to_datetime(df['sale_date'], utc=True)\n",
    "    df['treatment'] = df['treatment'].fillna('Unknown')\n",
    "    df['rarity'] = df['rarity'].fillna('Unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "sales_df = fetch_sales_data(days=180)\n",
    "print(f\"Loaded {len(sales_df):,} sales\")\n",
    "print(f\"Cards: {sales_df['card_id'].nunique()}\")\n",
    "print(f\"Date range: {sales_df['sale_date'].min()} to {sales_df['sale_date'].max()}\")\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_active_listings() -> pd.DataFrame:\n",
    "    \"\"\"Fetch current active listings for order book depth.\"\"\"\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(days=30)\n",
    "    \n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            card_id,\n",
    "            COALESCE(NULLIF(product_subtype, ''), treatment) as treatment,\n",
    "            COUNT(*) as listing_count,\n",
    "            MIN(price) as lowest_ask,\n",
    "            MAX(price) as highest_ask,\n",
    "            AVG(price) as avg_ask,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY price) as median_ask\n",
    "        FROM marketprice\n",
    "        WHERE listing_type = 'active'\n",
    "          AND is_bulk_lot = FALSE\n",
    "          AND scraped_at >= :cutoff\n",
    "        GROUP BY card_id, COALESCE(NULLIF(product_subtype, ''), treatment)\n",
    "    \"\"\")\n",
    "    \n",
    "    with Session(engine) as session:\n",
    "        result = session.execute(query, {\"cutoff\": cutoff})\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        'card_id', 'treatment', 'listing_count', 'lowest_ask', \n",
    "        'highest_ask', 'avg_ask', 'median_ask'\n",
    "    ])\n",
    "    df['treatment'] = df['treatment'].fillna('Unknown')\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['ask_spread'] = (df['highest_ask'] - df['lowest_ask']) / df['lowest_ask'].replace(0, np.nan)\n",
    "    df['ask_spread'] = df['ask_spread'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "listings_df = fetch_active_listings()\n",
    "print(f\"Loaded {len(listings_df):,} card/treatment combinations with active listings\")\n",
    "listings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Build features for each sale that capture:\n",
    "- Auto-regressive features (past prices)\n",
    "- Treatment/rarity effects\n",
    "- Liquidity metrics\n",
    "- Related card prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add auto-regressive features: lagged prices and rolling stats.\"\"\"\n",
    "    df = df.sort_values(['card_id', 'treatment', 'sale_date']).copy()\n",
    "    \n",
    "    # Group by card + treatment\n",
    "    group_cols = ['card_id', 'treatment']\n",
    "    \n",
    "    # Lag features (previous sales)\n",
    "    df['price_lag1'] = df.groupby(group_cols)['price'].shift(1)\n",
    "    df['price_lag2'] = df.groupby(group_cols)['price'].shift(2)\n",
    "    df['price_lag3'] = df.groupby(group_cols)['price'].shift(3)\n",
    "    \n",
    "    # Rolling statistics (last N sales)\n",
    "    df['price_roll_mean_5'] = df.groupby(group_cols)['price'].transform(\n",
    "        lambda x: x.shift(1).rolling(5, min_periods=1).mean()\n",
    "    )\n",
    "    df['price_roll_std_5'] = df.groupby(group_cols)['price'].transform(\n",
    "        lambda x: x.shift(1).rolling(5, min_periods=2).std()\n",
    "    )\n",
    "    df['price_roll_min_4'] = df.groupby(group_cols)['price'].transform(\n",
    "        lambda x: x.shift(1).rolling(4, min_periods=1).min()\n",
    "    )  # Floor-like feature\n",
    "    \n",
    "    # Price momentum (change from previous sale)\n",
    "    df['price_momentum'] = df['price_lag1'] - df['price_lag2']\n",
    "    df['price_pct_change'] = (df['price_lag1'] - df['price_lag2']) / df['price_lag2'].replace(0, np.nan)\n",
    "    \n",
    "    # Sale sequence number (experience with this card/treatment)\n",
    "    df['sale_seq'] = df.groupby(group_cols).cumcount()\n",
    "    \n",
    "    return df\n",
    "\n",
    "sales_df = compute_ar_features(sales_df)\n",
    "print(f\"AR features added\")\n",
    "sales_df[['card_name', 'treatment', 'price', 'price_lag1', 'price_roll_mean_5', 'price_roll_min_4']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_liquidity_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add liquidity metrics: sales velocity, time gaps.\"\"\"\n",
    "    df = df.sort_values(['card_id', 'treatment', 'sale_date']).copy()\n",
    "    group_cols = ['card_id', 'treatment']\n",
    "    \n",
    "    # Days since previous sale\n",
    "    df['prev_sale_date'] = df.groupby(group_cols)['sale_date'].shift(1)\n",
    "    df['days_since_last_sale'] = (df['sale_date'] - df['prev_sale_date']).dt.total_seconds() / 86400\n",
    "    \n",
    "    # Sales count in last 30 days (proxy for liquidity)\n",
    "    def count_recent_sales(group):\n",
    "        result = []\n",
    "        dates = group['sale_date'].values\n",
    "        for i, date in enumerate(dates):\n",
    "            cutoff = date - np.timedelta64(30, 'D')\n",
    "            count = np.sum((dates[:i] >= cutoff) & (dates[:i] < date))\n",
    "            result.append(count)\n",
    "        return pd.Series(result, index=group.index)\n",
    "    \n",
    "    df['sales_last_30d'] = df.groupby(group_cols, group_keys=False).apply(count_recent_sales)\n",
    "    \n",
    "    # Card-level total sales (popularity)\n",
    "    card_sales_count = df.groupby('card_id').size().rename('card_total_sales')\n",
    "    df = df.merge(card_sales_count, on='card_id', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "sales_df = compute_liquidity_features(sales_df)\n",
    "print(f\"Liquidity features added\")\n",
    "sales_df[['card_name', 'treatment', 'price', 'days_since_last_sale', 'sales_last_30d', 'card_total_sales']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_related_card_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add features from related cards (same rarity, same set).\"\"\"\n",
    "    \n",
    "    # Compute rarity-level price stats (excluding current card)\n",
    "    rarity_stats = df.groupby(['rarity', 'treatment']).agg({\n",
    "        'price': ['mean', 'median', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    rarity_stats.columns = ['rarity', 'treatment', 'rarity_mean', 'rarity_median', 'rarity_std', 'rarity_count']\n",
    "    \n",
    "    df = df.merge(rarity_stats, on=['rarity', 'treatment'], how='left')\n",
    "    \n",
    "    # Price relative to rarity average\n",
    "    df['price_vs_rarity'] = df['price'] / df['rarity_mean'].replace(0, np.nan)\n",
    "    \n",
    "    # Set-level stats\n",
    "    set_stats = df.groupby('set_name').agg({\n",
    "        'price': ['mean', 'median']\n",
    "    }).reset_index()\n",
    "    set_stats.columns = ['set_name', 'set_mean', 'set_median']\n",
    "    \n",
    "    df = df.merge(set_stats, on='set_name', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "sales_df = compute_related_card_features(sales_df)\n",
    "print(f\"Related card features added\")\n",
    "sales_df[['card_name', 'rarity', 'price', 'rarity_mean', 'price_vs_rarity']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_order_book_features(sales_df: pd.DataFrame, listings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge order book depth features.\"\"\"\n",
    "    \n",
    "    # Merge on card_id + treatment\n",
    "    sales_df = sales_df.merge(\n",
    "        listings_df[['card_id', 'treatment', 'listing_count', 'lowest_ask', 'avg_ask', 'ask_spread']],\n",
    "        on=['card_id', 'treatment'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing (no active listings)\n",
    "    sales_df['listing_count'] = sales_df['listing_count'].fillna(0)\n",
    "    sales_df['lowest_ask'] = sales_df['lowest_ask'].fillna(sales_df['price_lag1'])\n",
    "    sales_df['avg_ask'] = sales_df['avg_ask'].fillna(sales_df['price_lag1'])\n",
    "    sales_df['ask_spread'] = sales_df['ask_spread'].fillna(0)\n",
    "    \n",
    "    # Price vs lowest ask (deal indicator)\n",
    "    sales_df['price_vs_ask'] = sales_df['price'] / sales_df['lowest_ask'].replace(0, np.nan)\n",
    "    \n",
    "    return sales_df\n",
    "\n",
    "sales_df = add_order_book_features(sales_df, listings_df)\n",
    "print(f\"Order book features added\")\n",
    "sales_df[['card_name', 'treatment', 'price', 'listing_count', 'lowest_ask', 'ask_spread']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encode treatment and rarity as numeric features.\"\"\"\n",
    "    \n",
    "    # Treatment encoding (ordinal based on typical value)\n",
    "    treatment_order = {\n",
    "        'Classic Paper': 1,\n",
    "        'Classic Foil': 2,\n",
    "        'Stonefoil': 3,\n",
    "        'Serialized': 4,\n",
    "        'Unknown': 0,\n",
    "    }\n",
    "    df['treatment_code'] = df['treatment'].map(treatment_order).fillna(0)\n",
    "    \n",
    "    # Also create treatment multiplier (learned from data)\n",
    "    treatment_avg = df.groupby('treatment')['price'].mean()\n",
    "    base_price = treatment_avg.get('Classic Paper', treatment_avg.mean())\n",
    "    treatment_mult = (treatment_avg / base_price).to_dict()\n",
    "    df['treatment_mult'] = df['treatment'].map(treatment_mult).fillna(1.0)\n",
    "    \n",
    "    # Rarity encoding\n",
    "    rarity_order = {\n",
    "        'Common': 1,\n",
    "        'Uncommon': 2,\n",
    "        'Rare': 3,\n",
    "        'Epic': 4,\n",
    "        'Legendary': 5,\n",
    "        'Mythic': 6,\n",
    "        'Unknown': 0,\n",
    "    }\n",
    "    df['rarity_code'] = df['rarity'].map(rarity_order).fillna(0)\n",
    "    \n",
    "    # Rarity multiplier\n",
    "    rarity_avg = df.groupby('rarity')['price'].mean()\n",
    "    base_rarity = rarity_avg.get('Common', rarity_avg.mean())\n",
    "    rarity_mult = (rarity_avg / base_rarity).to_dict()\n",
    "    df['rarity_mult'] = df['rarity'].map(rarity_mult).fillna(1.0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "sales_df = encode_categorical_features(sales_df)\n",
    "print(f\"Categorical features encoded\")\n",
    "print(f\"\\nTreatment multipliers:\")\n",
    "print(sales_df.groupby('treatment')['treatment_mult'].first().sort_values())\n",
    "print(f\"\\nRarity multipliers:\")\n",
    "print(sales_df.groupby('rarity')['rarity_mult'].first().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add time-based features.\"\"\"\n",
    "    \n",
    "    df['day_of_week'] = df['sale_date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['hour'] = df['sale_date'].dt.hour\n",
    "    df['days_since_start'] = (df['sale_date'] - df['sale_date'].min()).dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "sales_df = add_temporal_features(sales_df)\n",
    "print(f\"Temporal features added\")\n",
    "print(f\"\\nFinal feature count: {len(sales_df.columns)}\")\n",
    "print(f\"Columns: {list(sales_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n",
    "\n",
    "Filter to rows with sufficient history and split train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns for model\n",
    "FEATURE_COLS = [\n",
    "    # AR features\n",
    "    'price_lag1', 'price_lag2', 'price_lag3',\n",
    "    'price_roll_mean_5', 'price_roll_std_5', 'price_roll_min_4',\n",
    "    'price_momentum', 'price_pct_change',\n",
    "    \n",
    "    # Liquidity\n",
    "    'days_since_last_sale', 'sales_last_30d', 'card_total_sales', 'sale_seq',\n",
    "    \n",
    "    # Related cards\n",
    "    'rarity_mean', 'rarity_median', 'set_mean',\n",
    "    \n",
    "    # Order book\n",
    "    'listing_count', 'lowest_ask', 'avg_ask', 'ask_spread',\n",
    "    \n",
    "    # Categorical\n",
    "    'treatment_code', 'treatment_mult', 'rarity_code', 'rarity_mult',\n",
    "    \n",
    "    # Temporal\n",
    "    'day_of_week', 'is_weekend',\n",
    "]\n",
    "\n",
    "TARGET_COL = 'price'\n",
    "\n",
    "# Filter rows with at least one previous sale (need lag features)\n",
    "model_df = sales_df[sales_df['sale_seq'] >= 1].copy()\n",
    "print(f\"Rows with history: {len(model_df):,} (dropped {len(sales_df) - len(model_df):,} first sales)\")\n",
    "\n",
    "# Drop rows with NaN in features\n",
    "model_df = model_df.dropna(subset=FEATURE_COLS + [TARGET_COL])\n",
    "print(f\"After dropping NaN: {len(model_df):,}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = model_df[FEATURE_COLS].values\n",
    "y = model_df[TARGET_COL].values\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (use recent data for testing)\n",
    "# Sort by date and use last 20% as test\n",
    "model_df = model_df.sort_values('sale_date')\n",
    "split_idx = int(len(model_df) * 0.8)\n",
    "\n",
    "train_df = model_df.iloc[:split_idx]\n",
    "test_df = model_df.iloc[split_idx:]\n",
    "\n",
    "X_train = train_df[FEATURE_COLS].values\n",
    "y_train = train_df[TARGET_COL].values\n",
    "X_test = test_df[FEATURE_COLS].values\n",
    "y_test = test_df[TARGET_COL].values\n",
    "\n",
    "print(f\"Train: {len(train_df):,} samples ({train_df['sale_date'].min()} to {train_df['sale_date'].max()})\")\n",
    "print(f\"Test:  {len(test_df):,} samples ({test_df['sale_date'].min()} to {test_df['sale_date'].max()})\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "Compare multiple model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        'Model': name,\n",
    "        'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'Train R²': r2_score(y_train, y_pred_train),\n",
    "        'Test R²': r2_score(y_test, y_pred_test),\n",
    "    }\n",
    "    \n",
    "    return results, model, y_pred_test\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    X_tr = X_train_scaled if 'Ridge' in name or 'Lasso' in name else X_train\n",
    "    X_te = X_test_scaled if 'Ridge' in name or 'Lasso' in name else X_test\n",
    "    \n",
    "    res, trained_model, y_pred = evaluate_model(name, model, X_tr, y_train, X_te, y_test)\n",
    "    results.append(res)\n",
    "    trained_models[name] = trained_model\n",
    "    predictions[name] = y_pred\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAE comparison\n",
    "ax1 = axes[0]\n",
    "x = range(len(results_df))\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], results_df['Train MAE'], width, label='Train', alpha=0.8)\n",
    "ax1.bar([i + width/2 for i in x], results_df['Test MAE'], width, label='Test', alpha=0.8)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('MAE ($)')\n",
    "ax1.set_title('Mean Absolute Error by Model')\n",
    "ax1.legend()\n",
    "\n",
    "# R² comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar([i - width/2 for i in x], results_df['Train R²'], width, label='Train', alpha=0.8)\n",
    "ax2.bar([i + width/2 for i in x], results_df['Test R²'], width, label='Test', alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('R² Score')\n",
    "ax2.set_title('R² Score by Model')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "Which features matter most for price prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best tree-based model\n",
    "best_model = trained_models.get('Gradient Boosting') or trained_models.get('Random Forest')\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': FEATURE_COLS,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(importance_df['Feature'], importance_df['Importance'], color='#4ecdc4')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Feature Importance (Gradient Boosting)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Features:\")\n",
    "    print(importance_df.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare to Current Floor Algorithm\n",
    "\n",
    "How does the ML model compare to simple avg-of-4-lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Use price_roll_min_4 (rolling min of last 4) as prediction\n",
    "baseline_pred = test_df['price_roll_min_4'].values\n",
    "\n",
    "# Also try price_lag1 as naive baseline\n",
    "naive_pred = test_df['price_lag1'].values\n",
    "\n",
    "# Best ML model\n",
    "best_model_name = results_df.loc[results_df['Test MAE'].idxmin(), 'Model']\n",
    "ml_pred = predictions[best_model_name]\n",
    "\n",
    "comparison = {\n",
    "    'Naive (Last Sale)': {\n",
    "        'MAE': mean_absolute_error(y_test, naive_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, naive_pred)),\n",
    "        'R²': r2_score(y_test, naive_pred),\n",
    "    },\n",
    "    'Floor (Min of 4)': {\n",
    "        'MAE': mean_absolute_error(y_test, baseline_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, baseline_pred)),\n",
    "        'R²': r2_score(y_test, baseline_pred),\n",
    "    },\n",
    "    f'ML ({best_model_name})': {\n",
    "        'MAE': mean_absolute_error(y_test, ml_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, ml_pred)),\n",
    "        'R²': r2_score(y_test, ml_pred),\n",
    "    },\n",
    "}\n",
    "\n",
    "comp_df = pd.DataFrame(comparison).T\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: ML vs Current Floor Algorithm\")\n",
    "print(\"=\"*60)\n",
    "print(comp_df.round(2).to_string())\n",
    "\n",
    "# Calculate improvement\n",
    "floor_mae = comparison['Floor (Min of 4)']['MAE']\n",
    "ml_mae = comparison[f'ML ({best_model_name})']['MAE']\n",
    "improvement = (floor_mae - ml_mae) / floor_mae * 100\n",
    "\n",
    "print(f\"\\nML improves MAE by {improvement:.1f}% over floor algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (name, pred) in zip(axes, [('Naive', naive_pred), ('Floor', baseline_pred), (f'ML', ml_pred)]):\n",
    "    ax.scatter(y_test, pred, alpha=0.3, s=10)\n",
    "    ax.plot([0, y_test.max()], [0, y_test.max()], 'r--', linewidth=2)\n",
    "    ax.set_xlabel('Actual Price')\n",
    "    ax.set_ylabel('Predicted Price')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xlim(0, np.percentile(y_test, 95))\n",
    "    ax.set_ylim(0, np.percentile(y_test, 95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test dataframe\n",
    "test_df = test_df.copy()\n",
    "test_df['ml_pred'] = ml_pred\n",
    "test_df['ml_error'] = test_df['ml_pred'] - test_df['price']\n",
    "test_df['ml_abs_error'] = np.abs(test_df['ml_error'])\n",
    "test_df['ml_pct_error'] = test_df['ml_abs_error'] / test_df['price'] * 100\n",
    "\n",
    "# Error by treatment\n",
    "print(\"Error by Treatment:\")\n",
    "treatment_errors = test_df.groupby('treatment').agg({\n",
    "    'ml_abs_error': ['mean', 'median'],\n",
    "    'ml_pct_error': 'median',\n",
    "    'price': 'count'\n",
    "}).round(2)\n",
    "treatment_errors.columns = ['MAE', 'Median AE', 'Median %Err', 'Count']\n",
    "print(treatment_errors.to_string())\n",
    "\n",
    "print(\"\\nError by Rarity:\")\n",
    "rarity_errors = test_df.groupby('rarity').agg({\n",
    "    'ml_abs_error': ['mean', 'median'],\n",
    "    'ml_pct_error': 'median',\n",
    "    'price': 'count'\n",
    "}).round(2)\n",
    "rarity_errors.columns = ['MAE', 'Median AE', 'Median %Err', 'Count']\n",
    "print(rarity_errors.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by price range\n",
    "test_df['price_bucket'] = pd.cut(\n",
    "    test_df['price'], \n",
    "    bins=[0, 10, 25, 50, 100, float('inf')],\n",
    "    labels=['$0-10', '$10-25', '$25-50', '$50-100', '$100+']\n",
    ")\n",
    "\n",
    "print(\"Error by Price Range:\")\n",
    "price_errors = test_df.groupby('price_bucket', observed=True).agg({\n",
    "    'ml_abs_error': ['mean', 'median'],\n",
    "    'ml_pct_error': 'median',\n",
    "    'price': 'count'\n",
    "}).round(2)\n",
    "price_errors.columns = ['MAE', 'Median AE', 'Median %Err', 'Count']\n",
    "print(price_errors.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Save best model\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "best_model = trained_models[best_model_name]\n",
    "model_path = model_dir / 'ar_pricing_model.joblib'\n",
    "scaler_path = model_dir / 'ar_pricing_scaler.joblib'\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Save feature list\n",
    "feature_path = model_dir / 'ar_pricing_features.txt'\n",
    "with open(feature_path, 'w') as f:\n",
    "    f.write('\\n'.join(FEATURE_COLS))\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "print(f\"Features saved to: {feature_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"AUTO-REGRESSIVE PRICING MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATA\")\n",
    "print(f\"   - Training samples: {len(train_df):,}\")\n",
    "print(f\"   - Test samples: {len(test_df):,}\")\n",
    "print(f\"   - Features: {len(FEATURE_COLS)}\")\n",
    "\n",
    "print(f\"\\n2. BEST MODEL: {best_model_name}\")\n",
    "print(f\"   - Test MAE: ${results_df[results_df['Model']==best_model_name]['Test MAE'].values[0]:.2f}\")\n",
    "print(f\"   - Test R²: {results_df[results_df['Model']==best_model_name]['Test R²'].values[0]:.3f}\")\n",
    "\n",
    "print(f\"\\n3. VS CURRENT FLOOR ALGORITHM\")\n",
    "print(f\"   - Floor MAE: ${floor_mae:.2f}\")\n",
    "print(f\"   - ML MAE: ${ml_mae:.2f}\")\n",
    "print(f\"   - Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\n4. TOP FEATURES\")\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_features = importance_df.tail(5)['Feature'].tolist()[::-1]\n",
    "    for i, f in enumerate(top_features, 1):\n",
    "        print(f\"   {i}. {f}\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS\")\n",
    "if improvement > 10:\n",
    "    print(\"   ✅ ML model significantly outperforms floor algorithm\")\n",
    "    print(\"   ✅ Consider deploying AR model for FMP calculation\")\n",
    "else:\n",
    "    print(\"   ⚠️ ML model only marginally better than floor algorithm\")\n",
    "    print(\"   ⚠️ Floor algorithm may be sufficient for most use cases\")\n",
    "\n",
    "print(\"   - Use treatment/rarity multipliers from training data\")\n",
    "print(\"   - Retrain weekly with new sales data\")\n",
    "print(\"   - Monitor for drift in prediction accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
